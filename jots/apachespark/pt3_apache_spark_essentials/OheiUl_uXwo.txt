{"text":"[{\"pos\":3879671,\"text\":\"<span class=\\\"t\\\">Hands on with RDDs</span>\"},{\"pos\":3914059,\"text\":\"/n//n/Let us take a look at the notebooks. \"},{\"pos\":3929110,\"text\":\"The lab is called washington-crime - an analysis of crime statistics from Washington DC using some publicly available data. \"},{\"pos\":3937958,\"text\":\"There is a lab notebook. \"},{\"pos\":3952240,\"text\":\"Try to create a cluster now if you haven't done it yet. \"},{\"pos\":3954496,\"text\":\"/n//n/It is self-directed. \"},{\"pos\":3965942,\"text\":\"You just execute code - select a cell and type SHIFT + ENTER. \"},{\"pos\":3985795,\"text\":\"/n//n/It created an RDD called baseRDD. \"},{\"pos\":3998645,\"text\":\"/n//n/You can peek what's in the RDD. \"},{\"pos\":4039861,\"text\":\"Print one per line for each of the first 10 records. \"},{\"pos\":4047851,\"text\":\"Take() is an action, it actually executed. \"},{\"pos\":4069694,\"text\":\"In the upper right, there is a question mark - shortcuts entry. \"},{\"pos\":4119868,\"text\":\"/n//n/Python strings have a u-prefix. Bottom line, ignore it for this lab. \"},{\"pos\":4222053,\"text\":\"/n//n/This demonstrates the filter transformation. \"},{\"pos\":4231436,\"text\":\"/n//n/Scala has something called the case class. \"},{\"pos\":4259359,\"text\":\"In Python, a named tuple allows access by names instead of indexes. \"},{\"pos\":4278162,\"text\":\"The Scala case class is similar to the Python named tuple. \"},{\"pos\":4360592,\"text\":\"/n//n/Use your language guide first before jumping to the solutions notebook. \"},{\"pos\":4403138,\"text\":\"/n//n/Couple of things: we are getting great questions already. \"},{\"pos\":4459804,\"text\":\"<span class=\\\"t\\\">About the Q &amp; A parking lot</span>\"},{\"pos\":4514238,\"text\":\"<span class=\\\"t\\\">Getting the slides for the presentation</span>\"},{\"pos\":4578691,\"text\":\"<span class=\\\"t\\\">Resources</span>\"},{\"pos\":4585048,\"text\":\"The Scala language guide has the Spark programming guide - a high level, introductory overview. \"},{\"pos\":4603697,\"text\":\"There is a second link, if you scroll down under API docs, RDDs common interface gives very detailed documentation about all the transformations and all the actions. \"},{\"pos\":4624680,\"text\":\"Quick tip: there is a text box at the top where you can type in a word and it will collapse and only show you those methods that match the pattern. \"},{\"pos\":4652347,\"text\":\"/n//n/We are going to move on. \"},{\"pos\":4695991,\"text\":\"<span class=\\\"t\\\">Intro to DataFrames and Spark SQL</span>\"},{\"pos\":4713433,\"text\":\"In your day to day work, you will start not with RDDs. \"},{\"pos\":4723675,\"text\":\"My suggestion is to start with DataFrames. \"},{\"pos\":4735278,\"text\":\"You don't have to delve into lower level details. \"},{\"pos\":4746199,\"text\":\"<span class=\\\"t\\\">Spark SQL</span>\"},{\"pos\":4751251,\"text\":\"Spark SQL is part of Spark itself. \"},{\"pos\":4762846,\"text\":\"It allows you to do queries and operations on distributed sets of data using more of a SQL like syntax. \"},{\"pos\":4782243,\"text\":\"<span class=\\\"t\\\">DataFrames API</span>\"},{\"pos\":4785801,\"text\":\"It is exposed through an API called DataFrames. \"},{\"pos\":4792428,\"text\":\"The DataFrame API has two motivations - we wanted to make it so that many people out there who already know SQL very well. We wanted to let people leverage that knowledge. \"},{\"pos\":4830139,\"text\":\"It is inspired by data frames in R and Python (Pandas). \"},{\"pos\":4879173,\"text\":\"It is built on RDD under the hood. \"},{\"pos\":4883485,\"text\":\"<span class=\\\"t\\\">DataFrames</span>\"},{\"pos\":4890285,\"text\":\"My advice is to start with DataFrames. \"},{\"pos\":4940992,\"text\":\"DataFrames are very useful not just because they are a simpler interface, but also the fact that there is an optimization layer called Catalyst - a subsystem in Spark which takes your specification and does many optimizations that RDDs themselves cannot do. \"},{\"pos\":4975360,\"text\":\"<span class=\\\"t\\\">DataFrames features</span>\"},{\"pos\":4980533,\"text\":\"Immutable just like RDDs. \"},{\"pos\":4994974,\"text\":\"It has a good amount of built in support for different data formats. \"},{\"pos\":5047045,\"text\":\"There is a very consistent API for DataFrames in different languages. \"},{\"pos\":5059161,\"text\":\"<span class=\\\"t\\\">DataFrames vs RDDs</span>\"},{\"pos\":5071450,\"text\":\"<span class=\\\"t\\\">Write less code: input and output</span>\"},{\"pos\":5081428,\"text\":\"Unified interface to reading/writing data in a variety of formats. \"},{\"pos\":5127894,\"text\":\"read() and write() functions create new builders for doing I/O. \"},{\"pos\":5202167,\"text\":\"<span class=\\\"t\\\">Data sources supported by DataFrames</span>\"},{\"pos\":5203881,\"text\":\"Built in - parquet, JDBC, JSON, MySQL, PostgreSQL, HDFS, S3\"},{\"pos\":5205235,\"text\":\"/n/External - CSV, dBase, elasticsearch, cassandra,redshift using a package. \"},{\"pos\":5239963,\"text\":\"<span class=\\\"t\\\">Write less code: high level operations</span>\"},{\"pos\":5248997,\"text\":\"Solve common problems concisely with DataFrames functions:\"},{\"pos\":5263397,\"text\":\"/n/- selecting columns and filtering\"},{\"pos\":5263935,\"text\":\"/n/- joining different data sources\"},{\"pos\":5263936,\"text\":\"/n/- aggregation (count, sum, average etc)\"},{\"pos\":5263937,\"text\":\"/n/- plotting results (e.g. with Pandas)\"},{\"pos\":5323540,\"text\":\"<span class=\\\"t\\\">Write less code: compute an average</span>\"},{\"pos\":5335205,\"text\":\"MapReduce code in Java. \"},{\"pos\":5387508,\"text\":\"The Spark code - in RDDs is less readable. \"},{\"pos\":5394195,\"text\":\"But Spark code in DataFrames - is more readable. \"},{\"pos\":5442039,\"text\":\"<span class=\\\"t\\\">Hands on</span>\"},{\"pos\":5448329,\"text\":\"Go into Sql-and-dataframes. \"},{\"pos\":5504713,\"text\":\"/n//n/Connect to your cluster. \"},{\"pos\":5770625,\"text\":\"<span class=\\\"t\\\">Construct a DataFrame (Scala)</span>\"},{\"pos\":5785530,\"text\":\"Spark SQL integrates very well with Hive. \"},{\"pos\":5799211,\"text\":\"Construct a dataframe from a Hive table using the table() method on sqlContext. \"},{\"pos\":5826328,\"text\":\"You can load from S3 also. \"},{\"pos\":5837330,\"text\":\"<span class=\\\"t\\\">Use DataFrames (Scala)</span>\"},{\"pos\":5845904,\"text\":\"DataFrames are immutable like RDDs. \"},{\"pos\":5853276,\"text\":\"Every transformation will create a new DataFrame, which itself will be immutable. \"},{\"pos\":5857618,\"text\":\"These are some examples. \"},{\"pos\":5865248,\"text\":\"/n//n/The argument to filter is a strange syntax. \"},{\"pos\":5898840,\"text\":\"What we have with the $\\\"age\\\" - that references a column in the DataFrame. \"},{\"pos\":5915229,\"text\":\"When we do filter(), we pass in a predicate (boolean true or false) expression - age is less than 21. \"},{\"pos\":5935640,\"text\":\"/n//n/DataFrames have three parts - the dataframe API, operations on column objects (like filter). \"},{\"pos\":6061224,\"text\":\"<span class=\\\"t\\\">Use DataFrames (Python)</span>\"},{\"pos\":6069550,\"text\":\"Analogous, syntax is different. \"},{\"pos\":6076740,\"text\":\"Age filter is like a dictionary. \"},{\"pos\":6184643,\"text\":\"<span class=\\\"t\\\">DataFrames and Spark SQL</span>\"},{\"pos\":6186742,\"text\":\"You can create tables also. \"},{\"pos\":6192305,\"text\":\"They have an SQL like interface for working with dataframes. \"},{\"pos\":6207873,\"text\":\"<span class=\\\"t\\\">Transformations, actions, laziness</span>\"},{\"pos\":6220396,\"text\":\"Like RDDs, dataframes are lazy. \"},{\"pos\":6231028,\"text\":\"Transformation contribute to the query plan, but don't execute anything. \"},{\"pos\":6245854,\"text\":\"Actions cause the execution of the query. \"},{\"pos\":6259509,\"text\":\"/n//n/Nothing actually happens until you run an action. \"},{\"pos\":6273486,\"text\":\"Transformations on RDDs on dataframes, the driver doesn't do anything. \"},{\"pos\":6301196,\"text\":\"<span class=\\\"t\\\">Creating a DataFrame in Scala</span>\"},{\"pos\":6370105,\"text\":\"A dataset is more important in Scala than Python. \"},{\"pos\":6379200,\"text\":\"It is a generalization of a dataframe. \"},{\"pos\":6414533,\"text\":\"<span class=\\\"t\\\">Creating a dataframe in Python</span>\"},{\"pos\":6418341,\"text\":\"Very similar syntax. \"},{\"pos\":6420796,\"text\":\"<span class=\\\"t\\\">DataFrames have Schemas</span>\"},{\"pos\":6429671,\"text\":\"The schema will be inferred if the loading data format does not explicitly support it. \"},{\"pos\":6435322,\"text\":\"If it does, it will use the defined format. \"},{\"pos\":6459376,\"text\":\"<span class=\\\"t\\\">printSchema()</span>\"},{\"pos\":6464771,\"text\":\"You can ask it what schema do you think you are?\"},{\"pos\":6490416,\"text\":\"<span class=\\\"t\\\">Schema Inference Example</span>\"},{\"pos\":6498395,\"text\":\"You can be more explicit about it. \"},{\"pos\":6516578,\"text\":\"/n//n/One strategy is to load the data as an RDD, do a transformation, and use a case class in Scala or named tuple in Python and package it into that. \"},{\"pos\":6539305,\"text\":\"Then you convert it to a DataFrame called toDF. \"},{\"pos\":6558287,\"text\":\"<span class=\\\"t\\\">Schema Inference (Python)</span>\"},{\"pos\":6564173,\"text\":\"<span class=\\\"t\\\">Schema Inference</span>\"},{\"pos\":6591085,\"text\":\"Scala - if you don't supply the column names, the API defaults to _1, _2 etc. \"},{\"pos\":6653447,\"text\":\"<span class=\\\"t\\\">What can I do with a DataFrame?</span>\"},{\"pos\":6663125,\"text\":\"We need to talk about columns. \"},{\"pos\":6668157,\"text\":\"<span class=\\\"t\\\">Columns</span>\"},{\"pos\":6671901,\"text\":\"You create column objects. \"},{\"pos\":6678857,\"text\":\"It is an abstraction. \"},{\"pos\":6689945,\"text\":\"/n//n/Pay attention - when you use and dont use strings to indicate column names. \"},{\"pos\":6701412,\"text\":\"Syntax for column objects depends on language - Scala uses the $ syntax. \"},{\"pos\":6776263,\"text\":\"/n//n/We recommend you use the dictionary like syntax for Python. \"},{\"pos\":6799199,\"text\":\"<span class=\\\"t\\\">show()</span>\"},{\"pos\":6819034,\"text\":\"RDD has take. \"},{\"pos\":6824618,\"text\":\"The corresponding thing in dataframes is show() - it is also an action. \"},{\"pos\":6864012,\"text\":\"/n//n/This is what it looks like. \"},{\"pos\":6871933,\"text\":\"<span class=\\\"t\\\">select()</span>\"},{\"pos\":6875604,\"text\":\"Very important to use this to constrain - tell Spark - what data you need and what you don't. \"},{\"pos\":6889288,\"text\":\"The names which are similar to corresponding concepts in SQL are probably the same. \"},{\"pos\":6893009,\"text\":\"/n//n/You can use complex expressions. \"}]","css":".t{font-weight:bold;}.t:before{content:\"\\A\\A\";white-space: pre;}.t:after{content:\"\\A\";white-space: pre;}","videoid":"OheiUl_uXwo","title":"Training  Apache Spark Essentials","duration":7259.101,"category":"Apache Spark","pageName":""}