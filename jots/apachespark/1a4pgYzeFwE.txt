{"text":"[{\"pos\":7470,\"text\":\"Today I want to talk about this concept that Matei started to introduce. The idea is structure in Spark. \"},{\"pos\":16521,\"text\":\"This is something we have been doing since we introduced SparkSQL into the Spark stack. \"},{\"pos\":20141,\"text\":\"We started with SQL, moved on to DataFrames, DataSets and now we are doing streaming. \"},{\"pos\":27082,\"text\":\"I want to discuss what I mean when I talk about structure and why structure is important. \"},{\"pos\":43681,\"text\":\"<span class=\\\"t\\\">Background: What is an RDD?</span>\"},{\"pos\":46341,\"text\":\"Dependencies\"},{\"pos\":74802,\"text\":\"/n/Partitions (with optional locality info)\"},{\"pos\":100963,\"text\":\"/n/Compute function: partition =&gt; Iterator[T]\"},{\"pos\":121129,\"text\":\"/n//n/This is a pretty simple API and that is where its power lies. \"},{\"pos\":125513,\"text\":\"In fact all of SparkSQL and dataframes etc are all implemented on top of the RDD API. \"},{\"pos\":137156,\"text\":\"/n//n/What is wrong with it? The computation itself is opaque. \"},{\"pos\":144413,\"text\":\"Its a blackbox. \"},{\"pos\":155272,\"text\":\"And the data itself is also opaque. \"},{\"pos\":175475,\"text\":\"As a result, the kinds of optimizations you can do are pretty limited. \"},{\"pos\":183801,\"text\":\"<span class=\\\"t\\\">Structure</span>\"},{\"pos\":190343,\"text\":\"Common patterns of data analysis. \"},{\"pos\":215823,\"text\":\"<span class=\\\"t\\\">Why structure?</span>\"},{\"pos\":222239,\"text\":\"By definition, structure will limit what can be expressed\"},{\"pos\":227945,\"text\":\"/n/In practice, we can accommodate the vast majority of computations within the framework\"},{\"pos\":238900,\"text\":\"/n//n/If computation falls outside the pattern, you can always use the escape hatch and go back to RDDs. \"},{\"pos\":247909,\"text\":\"Personally, I find myself going that less and less as the APIs get better and better. \"},{\"pos\":254159,\"text\":\"/n//n/The key idea here is - by limiting the space of what can be expressed, we enable all kinds of cool optimizations. \"},{\"pos\":267471,\"text\":\"For the rest of this talk, I will describe what Spark is able to understand about your computation and how it affects the performance of your job. \"},{\"pos\":276569,\"text\":\"<span class=\\\"t\\\">Structured APIs in Spark</span>\"},{\"pos\":290373,\"text\":\"SQL - plain string of text. No errors until runtime. \"},{\"pos\":319878,\"text\":\"At the same time, its a very useful language. \"},{\"pos\":327173,\"text\":\"/n/DataFrames - one step closer to type safe Java or Scala type of programming. \"},{\"pos\":332393,\"text\":\"Compiler errors - will show syntax errors. \"},{\"pos\":355447,\"text\":\"Asking for a column that does not exist will not throw error until runtime. \"},{\"pos\":361490,\"text\":\"/n/Datasets - lot more like RDDs. Typed object that programming language understands. If you call methods on your objects that don't exist or you mix up types, you will find all those things at compile time. \"},{\"pos\":382402,\"text\":\"/n//n/There is no one right answer. It is about what computation you are doing and how you can express it most easily. \"},{\"pos\":390528,\"text\":\"<span class=\\\"t\\\">Datasets API</span>\"},{\"pos\":395906,\"text\":\"Type safe objects - operate on them with lambda functions. \"},{\"pos\":411108,\"text\":\"/n//n/Applying a class to data read from json. \"},{\"pos\":440399,\"text\":\"We support nesting and arrays and all kinds of things inside dataset classes. \"},{\"pos\":446970,\"text\":\"/n//n/Once it is in the form of an object, I can pass a lambda function for example. \"},{\"pos\":457188,\"text\":\"Also very useful if you have code that is hard to express in SQL. \"},{\"pos\":461267,\"text\":\"For example, group by name and compute histograms. \"},{\"pos\":486302,\"text\":\"<span class=\\\"t\\\">DataFrame = Dataset[Row]</span>\"},{\"pos\":492758,\"text\":\"Something which has changed from Spark 1.6. \"},{\"pos\":505167,\"text\":\"We have unified them in 2.x. \"},{\"pos\":533671,\"text\":\"/n/A dataframe is just a dataset of generic row objects. \"},{\"pos\":537335,\"text\":\"When you don't know all of the fields ahead of time, generic row objects are the answer. \"},{\"pos\":545800,\"text\":\"Anytime you use stringly-typed methods it will just return DataFrame and you can always switch back to the type representation by using as keyword. \"},{\"pos\":561238,\"text\":\"<span class=\\\"t\\\">What about python?</span>\"},{\"pos\":568370,\"text\":\"Because of dynamic typing, you have always had a lot of benefits of the dataset API. \"},{\"pos\":594467,\"text\":\"There is not really a lot of reason to have this, although we are continuing to look at ways to make sure all of the APIs are consistent and easy to use. \"},{\"pos\":608320,\"text\":\"<span class=\\\"t\\\">Shared optimization and execution</span>\"},{\"pos\":611018,\"text\":\"The key idea here is no matter which language or API you use, you are constructing a logical query plan. \"},{\"pos\":625088,\"text\":\"That is what tells Spark the structure in your application. \"},{\"pos\":635018,\"text\":\"The optimization pipeline is completely agnostic to where that logical query plan came from. \"},{\"pos\":642236,\"text\":\"<span class=\\\"t\\\">Structuring computation</span>\"},{\"pos\":654611,\"text\":\"<span class=\\\"t\\\">Columns</span>\"},{\"pos\":659634,\"text\":\"Very important part of dataset and dataframe APIs. \"},{\"pos\":662246,\"text\":\"A way to express a new value based on input values. \"},{\"pos\":696508,\"text\":\"/n//n/expr is basically an escape hatch into a full fledged SQL parser that you can use from inside of dataframe API. \"},{\"pos\":702585,\"text\":\"Anytime you know the SQL way to do it, you can type arbitrary SQL statements. \"},{\"pos\":721984,\"text\":\"Underneath the covers all of them will produce an expression to check to see if x equals 1. \"},{\"pos\":726188,\"text\":\"<span class=\\\"t\\\">Complex columns with functions</span>\"},{\"pos\":728481,\"text\":\"Its not just about very simple things like equality. \"},{\"pos\":730891,\"text\":\"Since Spark 1.5 we have a pretty rich library of functions you can actually compose. \"},{\"pos\":736057,\"text\":\"The cool thing is - you are not calling the function at analysis time on your driver machine. \"},{\"pos\":747472,\"text\":\"You are constructing instructions for the query planner about how to compose these operations and then at runtime it is going to generate efficient bytecode for the specific functions that you are calling. \"},{\"pos\":754828,\"text\":\"So all of these functions have built-in code generation capabilities. \"},{\"pos\":767497,\"text\":\"/n//n/Pretty common question - what is the performance hit for writing in Python instead of Scala? \"},{\"pos\":773430,\"text\":\"If you use these functions, its the same. \"},{\"pos\":782139,\"text\":\"<span class=\\\"t\\\">Functions and Columns</span>\"},{\"pos\":820842,\"text\":\"<span class=\\\"t\\\">Columns: predicate pushdown</span>\"},{\"pos\":834770,\"text\":\"On top of a dataframe, I apply a predicate - I only want records where name is michael. \"},{\"pos\":840761,\"text\":\"Under the covers, Spark understands the equality and generates very efficient SQL and takes the predicate and pushes it down into the PostGRES database. \"},{\"pos\":853930,\"text\":\"No records which don't match the predicate will be even /i/shipped/ over the network. \"},{\"pos\":858659,\"text\":\"Understanding allows us to shift computation around in more intelligent ways. \"},{\"pos\":862197,\"text\":\"<span class=\\\"t\\\">Columns: Efficient joins</span>\"},{\"pos\":865508,\"text\":\"I am doing a join where I have a UDF which is checking if two columns are equal. \"},{\"pos\":871366,\"text\":\"When I apply that filter, the only way for Spark to know which tuples are matching your predicate is to try every single possible combination which is called the cartesian product and pass that to your function. \"},{\"pos\":891614,\"text\":\"For two large tables it is incredibly expensive. \"},{\"pos\":895702,\"text\":\"/n//n/Instead if you specified it as a column expression, we are going to take the data and do a hash partition across the cluster using a shuffle and then we will build hash tables on either side to make sure that we can do this efficiently. \"},{\"pos\":907962,\"text\":\"It will be nlogn, not n^2. \"},{\"pos\":934273,\"text\":\"<span class=\\\"t\\\">Spark's structured data model</span>\"},{\"pos\":934274,\"text\":\"Primitives - byte, short, integer\"},{\"pos\":946350,\"text\":\"/n/Array[Type]\"},{\"pos\":954308,\"text\":\"/n/Struct\"},{\"pos\":957319,\"text\":\"/n/Map[Type,Type]\"},{\"pos\":962528,\"text\":\"/n//n/By composing these, you can pretty much model many of the different cases - Avro, Parquet, simple case classes, Java Beans. \"},{\"pos\":972523,\"text\":\"As a result, we can do a pretty easy mapping between these other types of data and the Spark data model. \"},{\"pos\":977665,\"text\":\"<span class=\\\"t\\\">Tungsten's compact encoding</span>\"},{\"pos\":978290,\"text\":\"The important thing is we can leverage that when we do encoding. \"},{\"pos\":987136,\"text\":\"We take data, cache it in memory, shuffle it over network etc. \"},{\"pos\":1005772,\"text\":\"<span class=\\\"t\\\">Encoders</span>\"},{\"pos\":1009148,\"text\":\"A new concept - but very important. \"},{\"pos\":1023693,\"text\":\"We automatically take the structure of your data and do code generation at runtime to build these encoders to do these types of translations. \"},{\"pos\":1033009,\"text\":\"<span class=\\\"t\\\">Bridge objects with data sources</span>\"},{\"pos\":1046125,\"text\":\"Any of the different data sources that Spark supports. \"},{\"pos\":1058689,\"text\":\"<span class=\\\"t\\\">Space efficiency</span>\"},{\"pos\":1063200,\"text\":\"It actually turns out, you also end up with significant space efficiency. \"},{\"pos\":1087401,\"text\":\"The same datasets need significantly more memory to save as RDD. \"},{\"pos\":1096883,\"text\":\"<span class=\\\"t\\\">Serialization performance</span>\"},{\"pos\":1099169,\"text\":\"And its not only about the size when you are storing in memory, its also about the performance when going back and forth between these representations. \"},{\"pos\":1105263,\"text\":\"A pretty common tuning tip is to take out your Java serialization and switch it out with Kryo. \"},{\"pos\":1121792,\"text\":\"Even when compared with Kryo, these encoders are significantly faster. \"},{\"pos\":1134782,\"text\":\"This is because we do this at the last possible minute and build custom bytecode for that specific object. \"},{\"pos\":1144982,\"text\":\"<span class=\\\"t\\\">Operate directly on serialized data</span>\"},{\"pos\":1166051,\"text\":\"Take a dataframe and find all the records where the year &gt; 2015. \"},{\"pos\":1178272,\"text\":\"Spark will turn it into a Catalyst expression. \"},{\"pos\":1180678,\"text\":\"Catalyst is the name of the query optimizer that runs inside of Spark. \"},{\"pos\":1194874,\"text\":\"Spark translates this into Java code, give it to a Java compiler and we get back bytecode that we actually execute. \"},{\"pos\":1214215,\"text\":\"/n//n/We actually know exactly where we need to jump to in order to find it. \"},{\"pos\":1216046,\"text\":\"And then we pull out the data and do the operation we want. \"},{\"pos\":1219680,\"text\":\"There is no allocation, you created no objects, no GC pressure and as a result will run significantly faster than if it was your own lambda function. \"},{\"pos\":1234768,\"text\":\"An important detail - Platform.getInt() is called a JVM intrinsic. \"},{\"pos\":1240496,\"text\":\"It will JIT down to assembly which is pretty close to doing pointer arithmetic. \"},{\"pos\":1244975,\"text\":\"We took our code generator and compared it with hand written C++ code, it is actually within 10-15%. \"},{\"pos\":1258182,\"text\":\"This is pretty close to bare metal without writing anything more than dataframe code. \"},{\"pos\":1267546,\"text\":\"<span class=\\\"t\\\">Structured streaming</span>\"},{\"pos\":1275177,\"text\":\"Couple of streaming APIs in Spark. It was very low level RDD focused. \"},{\"pos\":1291844,\"text\":\"We want to use nice, structured APIs and do streaming on top of that. \"},{\"pos\":1298862,\"text\":\"/n//n/The simplest way to perform streaming analytics is not having to /i/reason/ about streaming. \"},{\"pos\":1317333,\"text\":\"/n//n/So you end up with an API which is very similar to what you had before. \"},{\"pos\":1321168,\"text\":\"You had continuous dataframes - which are just dataframes where data continues to arrive over time and we process it incrementally as it gets there. \"},{\"pos\":1335213,\"text\":\"<span class=\\\"t\\\">Structured streaming</span>\"},{\"pos\":1341854,\"text\":\"High level streaming API built on Apache Spark SQL engine\"},{\"pos\":1363346,\"text\":\"/n/There is an unsupported operation checker. \"},{\"pos\":1404047,\"text\":\"/n/Unifies streaming, interactive and batch queries\"},{\"pos\":1423687,\"text\":\"/n/You can change queries at runtime\"},{\"pos\":1444175,\"text\":\"/n/Since Spark has long had this vision of being a unified stack, we are working very closely with the ML team to figure out how we can make it so that all ML modes that do online streaming can be seamlessly applied in Spark streaming\"},{\"pos\":1461674,\"text\":\"<span class=\\\"t\\\">Example: Batch Aggregation</span>\"},{\"pos\":1519433,\"text\":\"Instead of load, use keyword stream. \"},{\"pos\":1534245,\"text\":\"<span class=\\\"t\\\">Execution</span>\"},{\"pos\":1539600,\"text\":\"Logical operation: what the structure of your operation is. \"},{\"pos\":1551354,\"text\":\"And physically, we understand exactly how we are going to perform that optimization. \"},{\"pos\":1552645,\"text\":\"And the trick is we are trying to use the Catalyst optimizer and take that query plan and turn it into an incremental query plan. \"},{\"pos\":1560485,\"text\":\"<span class=\\\"t\\\">Incrementalized by Spark</span>\"},{\"pos\":1563040,\"text\":\"A relatively simple example here. \"},{\"pos\":1600440,\"text\":\"Scan new files, stateful aggregate, update MySQL. \"},{\"pos\":1605195,\"text\":\"<span class=\\\"t\\\">What's coming?</span>\"},{\"pos\":1631034,\"text\":\"Spark 2.0: unification of dataframe/dataset/context, basic streaming API, event time aggregations\"},{\"pos\":1652041,\"text\":\"/n/2.1 - other streaming sources and sinks, integration with machine learning and the concept of watermarks. \"},{\"pos\":1668978,\"text\":\"/n/More and more structure being applied to other libraries - MLLib pipelines, GraphFrames. \"}]","css":".t{font-weight:bold;}.t:before{content:\"\\A\\A\";white-space: pre;}.t:after{content:\"\\A\";white-space: pre;}","videoid":"1a4pgYzeFwE","title":"Structuring Spark: Dataframes, Datasets And Streaming","duration":1705.261,"category":"Apache Spark","pageName":""}