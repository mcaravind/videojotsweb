{"text":"[{\"pos\":5919,\"text\":\"My name is Will Benton and I am software engineer at Red Hat. \"},{\"pos\":7549,\"text\":\"Today I will talk about some work I have been doing recently to make sense of log data with Spark. \"},{\"pos\":45583,\"text\":\"<span class=\\\"t\\\">Challenges of log data</span>\"},{\"pos\":63426,\"text\":\"They are not explicitly structured, they come from many sources and they can be rich. \"},{\"pos\":66178,\"text\":\"Tricky to train predictive models from log data. \"},{\"pos\":67606,\"text\":\"This has increased recently. \"},{\"pos\":101265,\"text\":\"<span class=\\\"t\\\">Data ingest</span>\"},{\"pos\":106936,\"text\":\"<span class=\\\"t\\\">Collecting log data</span>\"},{\"pos\":119811,\"text\":\"Collectors take logs from source machines and convey them across the network. \"},{\"pos\":130117,\"text\":\"/n/We have normalizers which take raw log data and impose some metadata and other data hygiene standards on them. \"},{\"pos\":153661,\"text\":\"/n/Finally we store all this data in ElasticSearch instance. \"},{\"pos\":162466,\"text\":\"/n//n/We have a whole bunch of data in ElasticSearch including cloud infrastructure, application infrastructure, machines that run QA tests, performance evaluation machines, and individual workstations. \"},{\"pos\":175152,\"text\":\"/n//n/I took the data out of ElasticSearch and archived it to Parquet for my work. \"},{\"pos\":182255,\"text\":\"I use that to support predictive training models. \"},{\"pos\":218532,\"text\":\"<span class=\\\"t\\\">Schema mediation</span>\"},{\"pos\":228229,\"text\":\"Schemas change. \"},{\"pos\":235606,\"text\":\"Easy first pass is to take the intersection of everything you have. \"},{\"pos\":238618,\"text\":\"And in Spark this is really simple. \"},{\"pos\":266471,\"text\":\"/n//n/You have lots of fields from this - timestamp, level host, IP addresses, message etc. \"},{\"pos\":274725,\"text\":\"/n/And then additional metadata from the normalization process - environment variables, app name, facility name etc. \"},{\"pos\":291485,\"text\":\"/n//n/After you have a consistent structure you can write aggregate queries. \"},{\"pos\":295459,\"text\":\"You also have to make sure you don't have spurious NULL values. \"},{\"pos\":311381,\"text\":\"Spark will infer the broadest schema to accommodate everything you have. \"},{\"pos\":315905,\"text\":\"So some records will have fields with NULL values. \"},{\"pos\":318929,\"text\":\"Unfortunately if you are dealing with a lot of libraries they may not be coded defensively to deal with NULLs. \"},{\"pos\":327304,\"text\":\"So you want to sanitize them out. \"},{\"pos\":334837,\"text\":\"<span class=\\\"t\\\">Exploring structured data</span>\"},{\"pos\":345606,\"text\":\"On the top row, we are looking at severities.\"},{\"pos\":354970,\"text\":\"On the bottom row, we are querying what kind of applications generate the most of each kind of log message at a given severity. \"},{\"pos\":388963,\"text\":\"This new datasets API lets you write programs that benefit from type checking. \"},{\"pos\":399391,\"text\":\"And also from query planning. \"},{\"pos\":413450,\"text\":\"A nice side benefit of datasets is that they make this kind of code totally painless to write. \"},{\"pos\":442657,\"text\":\"<span class=\\\"t\\\">Feature engineering</span>\"},{\"pos\":470558,\"text\":\"Some data do not have no obvious ordering or numerical interpretation - e.g. severity, hostnames. \"},{\"pos\":482240,\"text\":\"<span class=\\\"t\\\">From log records to vectors</span>\"},{\"pos\":490509,\"text\":\"One hot encoding takes a finite domain of things and turns it into a vector of either n bits where exactly 1 bit is set or exactly n-1 bits where at most 1 bit is set. \"},{\"pos\":508762,\"text\":\"There is an encoder to do this in Spark's ML pipelines library. \"},{\"pos\":524827,\"text\":\"/n//n/We can use these to train models. \"},{\"pos\":542493,\"text\":\"<span class=\\\"t\\\">Similarity and distance</span>\"},{\"pos\":543276,\"text\":\"/n//n/Now that we have a vector of numbers we can talk about similarity. \"},{\"pos\":553059,\"text\":\"Often we are thinking about geometric notions of similarity - Euclidean distance for example. \"},{\"pos\":560291,\"text\":\"Manhattan distance. \"},{\"pos\":570182,\"text\":\"And finally angular similarity. \"},{\"pos\":582816,\"text\":\"This is really good for sparse binary vectors. \"},{\"pos\":589610,\"text\":\"<span class=\\\"t\\\">Other interesting features</span>\"},{\"pos\":594111,\"text\":\"Distribution of log records interarrival time - per host, per subsystem, per service. We can recognize something is happening to our system if log records start arriving significantly more quickly or slowly than in past. \"},{\"pos\":614051,\"text\":\"/n/Another feature is mismatch in natural language and logs. \"},{\"pos\":690907,\"text\":\"<span class=\\\"t\\\">Visualizing structure and finding outliers</span>\"},{\"pos\":694247,\"text\":\"How do we look at the structure of our logs as a whole? \"},{\"pos\":702547,\"text\":\"<span class=\\\"t\\\">Multidimensional data</span>\"},{\"pos\":724954,\"text\":\"When you get to more than 3 dimensions our intuition begins to break down. \"},{\"pos\":728227,\"text\":\"Most real world high dimensional data only have a few meaningful dimensions. \"},{\"pos\":742584,\"text\":\"Its easier for computers to find patterns in low dimensional representations just like humans. \"},{\"pos\":749363,\"text\":\"We should use some techniques which cut down the unimportant dimensions and leave us with the interesting ones. \"},{\"pos\":754019,\"text\":\"<span class=\\\"t\\\">A linear approach: PCA</span>\"},{\"pos\":762327,\"text\":\"Find a transformation from a matrix to a smaller matrix that preserves uncorrelated and high variance features from the source but discards low variance ones. \"},{\"pos\":772454,\"text\":\"This is available in Spark, old technique, and efficient to compute. \"},{\"pos\":782945,\"text\":\"/n//n/If you apply this to actual log data, you get something that looks a lot like clustering. \"},{\"pos\":794995,\"text\":\"There are a few downsides to PCA, most importantly it is not human interpretable. \"},{\"pos\":817122,\"text\":\"<span class=\\\"t\\\">Tree based approaches</span>\"},{\"pos\":825933,\"text\":\"The decision trees emphasizes the features that best differentiate between the objects and ignores features with low information. \"},{\"pos\":830368,\"text\":\"If you haven't trained decision trees before, think of how you would try and win a game of 20 questions.\"},{\"pos\":840073,\"text\":\"/n//n/This is a supervised learning technique. \"},{\"pos\":848504,\"text\":\"There is techniques for doing unsupervised learning - ensembles of trees - that have the same property of honing in on the most important features. \"},{\"pos\":858746,\"text\":\"By training families of trees to distinguish between real and synthetic data and using this decision to label. \"},{\"pos\":865151,\"text\":\"/n//n/Random forest clustering used on 4400 dimensional source data and the training process was able to identify clusters in this data using only 4 percent of those dimensions. \"},{\"pos\":880502,\"text\":\"<span class=\\\"t\\\">Self organizing maps</span>\"},{\"pos\":886765,\"text\":\"Technique that learns a low dimensional map of high dimensional objects. \"},{\"pos\":901069,\"text\":\"So that things that map to nearby entries on the low dimensional map are likely to be similar in the high dimensional space. \"},{\"pos\":927029,\"text\":\"/n//n/The cool thing about these is that you can also use them to find outliers in your data. \"},{\"pos\":929411,\"text\":\"After you have trained one of these maps, you have a collection of things that resemble the things you care about. \"},{\"pos\":954936,\"text\":\"<span class=\\\"t\\\">Outliers in log data</span>\"},{\"pos\":962817,\"text\":\"Two months of log data. \"},{\"pos\":985739,\"text\":\"/n//n/Most of the records matched well. \"},{\"pos\":991347,\"text\":\"But the ones which matched poorly matched really really poorly. \"},{\"pos\":992570,\"text\":\"If we consider outliers to be anything where the best match is atleast four standard deviations below the mean match we narrowed down the scope of things we have to look at quite a lot. \"},{\"pos\":1006323,\"text\":\"Out of 300 million log, about 0.0012% were identified as outliers. \"},{\"pos\":1027597,\"text\":\"/n//n/The really cool thing is - the most extreme outliers are all things you want to know about. \"},{\"pos\":1033814,\"text\":\"<span class=\\\"t\\\">SOM training in Spark</span>\"},{\"pos\":1056032,\"text\":\"<span class=\\\"t\\\">On-line SOM training</span>\"},{\"pos\":1057509,\"text\":\"The idea is - when you get an example object - you find the best match in the map and add that object to the nearby cells in the map. \"},{\"pos\":1070772,\"text\":\"Having it contribute a lot to the cells that are nearest to the best match and a little to the cells that are farther away. \"},{\"pos\":1093928,\"text\":\"/n//n/In Python-esque pseudocode. \"},{\"pos\":1133212,\"text\":\"/n//n/It has drawbacks - not parallel. \"},{\"pos\":1160834,\"text\":\"<span class=\\\"t\\\">Batch SOM training</span>\"},{\"pos\":1161240,\"text\":\"In Spark, keep a weighted average of all the matches we see in any given iteration. \"},{\"pos\":1172740,\"text\":\"And we build up an intermediate state. \"},{\"pos\":1191032,\"text\":\"/n//n/Picture explanation of pseudocode\"},{\"pos\":1247278,\"text\":\"/n//n/Actually implementing this in Spark, we can easily aggregate intermediate weighted averages from individual partitions. \"},{\"pos\":1296415,\"text\":\"/n//n/Problem: Your map does not have to be very large before you are taking a lot of data back to the driver. \"},{\"pos\":1301812,\"text\":\"If you have a 3MB model and 2048 partitions, thats 6GB at the driver. \"},{\"pos\":1320946,\"text\":\"A better way to do this is to use the tree aggregate. \"},{\"pos\":1330356,\"text\":\"It improves both time performance and space performance. \"},{\"pos\":1340590,\"text\":\"If we use tree aggregate, we do some work in a shuffle step and not as many things will go back to the driver. \"},{\"pos\":1356418,\"text\":\"<span class=\\\"t\\\">Sharing models beyond Spark</span>\"},{\"pos\":1368294,\"text\":\"You may need to train a model in Spark and use it from another system. \"},{\"pos\":1377634,\"text\":\"This can come up even if only using Spark - you have implemented the learning algorithm in Scala and want to make predictions with the model in Python. \"},{\"pos\":1456206,\"text\":\"<span class=\\\"t\\\">Practical matters</span>\"},{\"pos\":1459631,\"text\":\"<span class=\\\"t\\\">Spark and ElasticSearch</span>\"},{\"pos\":1460773,\"text\":\"Deciding when and what to cache is hard in any program. \"},{\"pos\":1465007,\"text\":\"It matters a lot more if the source is in another data center. \"},{\"pos\":1470373,\"text\":\"/n//n/If you are using a database just to support training models, consider exporting into Parquet locally and operating on that. \"},{\"pos\":1478815,\"text\":\"<span class=\\\"t\\\">Structured queries in Spark</span>\"},{\"pos\":1481011,\"text\":\"Program defensively. \"},{\"pos\":1481012,\"text\":\"<span class=\\\"t\\\">Memory and partitioning</span>\"},{\"pos\":1499872,\"text\":\"Compute machines with 100s of GBs of memory are common place but you cannot create a JVM that uses all of that. \"},{\"pos\":1505780,\"text\":\"Going forward you will be able to use off-heap storage in Spark 2.0. \"},{\"pos\":1509253,\"text\":\"You can run multiple JVMs with smaller heaps on a single machine to take advantage of lots of memory without overburdening the garbage collector. \"},{\"pos\":1521171,\"text\":\"/n//n/And you can often sidestep garbage collector issues altogether by using a more efficient approach that doesn't require you to allocate as much memory. \"},{\"pos\":1530460,\"text\":\"<span class=\\\"t\\\">Interoperability</span>\"},{\"pos\":1530775,\"text\":\"<span class=\\\"t\\\">Feature engineering</span>\"},{\"pos\":1537564,\"text\":\"Favor feature engineering over complex or novel learning algorithms. \"},{\"pos\":1539178,\"text\":\"/n//n/Value approaches that train interpretable models. \"},{\"pos\":1545088,\"text\":\"Making predictions is only part of the value, the other value is what it tells you about your data, what it tells you about the world that you got that data from. \"},{\"pos\":1568044,\"text\":\"<span class=\\\"t\\\">Questions</span>\"},{\"pos\":1573282,\"text\":\"<span class=\\\"t\\\">Are you using structured streaming API? How much work did you invest in converting the ingesting the ES output into Parquet files?</span>\"},{\"pos\":1595488,\"text\":\"The way it is structured now is we have one index for every day. \"},{\"pos\":1601521,\"text\":\"Thats a great thing to look at in the future. \"},{\"pos\":1612154,\"text\":\"<span class=\\\"t\\\">Exporting importing part of the model?</span>\"},{\"pos\":1624581,\"text\":\"The idea is if you have a Scala case class this json4s library will encode that as JSON for you which you can then load into Python. \"},{\"pos\":1639035,\"text\":\"<span class=\\\"t\\\">Is it possible to use that in other places?</span>\"},{\"pos\":1652800,\"text\":\"Thats the solution for serializing model parameters in a sensible way. \"},{\"pos\":1657233,\"text\":\"Actually implementing the technique to do something with those model parameters is another story. \"},{\"pos\":1681089,\"text\":\"<span class=\\\"t\\\">If you are elastic cluster is colocated with the Spark cluster, does it still make sense to export it into Parquet files and then run this?</span>\"},{\"pos\":1689291,\"text\":\"It is still faster to run from Parquet than it is to run from elastic, especially if you are not using all of elastic's capabilities. \"},{\"pos\":1714884,\"text\":\"<span class=\\\"t\\\">How do you detect timeline between anomalies?</span>\"},{\"pos\":1741427,\"text\":\"Thats a great question - we are looking into that. \"}]","css":".t{font-weight:bold;}.t:before{content:\"\\A\\A\";white-space: pre;}.t:after{content:\"\\A\";white-space: pre;}","videoid":"fhuoKe4li6E","title":"Analyzing Log Data With Apache Spark","duration":1759.281,"category":"Apache Spark","pageName":""}