{"text":"[{\"pos\":1687988,\"text\":\"<span class=\\\"t\\\">Spark</span>\"},{\"pos\":1692580,\"text\":\"Started as a research project at UC Berkeley in 2009\"},{\"pos\":1706109,\"text\":\"/n/Open source license\"},{\"pos\":1710644,\"text\":\"/n/Latest stable release v2.0 (june 2016)\"},{\"pos\":1719781,\"text\":\"/n/600,000 lines of code (75% Scala)\"},{\"pos\":1730074,\"text\":\"/n/Built by 800+ developers from 200+ companies\"},{\"pos\":1786131,\"text\":\"<span class=\\\"t\\\">Environment in which Spark came up</span>\"},{\"pos\":1797001,\"text\":\"Diagram\"},{\"pos\":1870203,\"text\":\"<span class=\\\"t\\\">Opportunity</span>\"},{\"pos\":1875114,\"text\":\"- Keep more data in memory\"},{\"pos\":1885748,\"text\":\"/n/- New distributed execution environment\"},{\"pos\":1892898,\"text\":\"/n/- Bindings for Python, Java, Scala, R\"},{\"pos\":1951608,\"text\":\"<span class=\\\"t\\\">MapReduce: Use Disk</span>\"},{\"pos\":1967274,\"text\":\"If you run MapReduce using HDFS, you have an iterative process of calculation. You serialize to disk. \"},{\"pos\":1991936,\"text\":\"When reading, you have a distributed read as well. \"},{\"pos\":1994975,\"text\":\"<span class=\\\"t\\\">Spark: In-memory data sharing</span>\"},{\"pos\":1996981,\"text\":\"Spark is architected a little differently. \"},{\"pos\":1999031,\"text\":\"It can write to disk when it needs to (spill to disk). \"},{\"pos\":2004387,\"text\":\"Normally, it can operate in memory which is faster. \"},{\"pos\":2032089,\"text\":\"<span class=\\\"t\\\">Goal: Unified engine across data sources</span>\"},{\"pos\":2052601,\"text\":\"You can do a lot of things in Spark. \"},{\"pos\":2067126,\"text\":\"If you learn one thing, and it lets you solve a lot of different problems, it is more valuable to you. \"},{\"pos\":2091279,\"text\":\"You can also utilize a lot of things you use today in your day to day in conjunction with Spark. \"},{\"pos\":2108070,\"text\":\"/n//n/In terms of execution environments, you can run standalone, AWS, Docker, YARN. \"},{\"pos\":2169827,\"text\":\"<span class=\\\"t\\\">End of Spark overview</span>\"},{\"pos\":2187922,\"text\":\"<span class=\\\"t\\\">RDD fundamentals</span>\"},{\"pos\":2251352,\"text\":\"Core abstraction is an RDD. \"},{\"pos\":2295449,\"text\":\"/n//n/There are several ways to interact with Spark. \"},{\"pos\":2304719,\"text\":\"Often you write Spark apps, you write a program which is compiled and then submitted. \"},{\"pos\":2313456,\"text\":\"There is an interactive mode as well, something called the spark-shell. \"},{\"pos\":2320186,\"text\":\"That is what I just started. \"},{\"pos\":2326461,\"text\":\"The python version is called pyspark. \"},{\"pos\":2335136,\"text\":\"Typically support features come first to Scala, but there is a tremendous amount of support and interest and demand for Python. \"},{\"pos\":2367161,\"text\":\"/n//n/I put together a language guide. \"},{\"pos\":2445110,\"text\":\"It is a reference to a lot of documentation. \"},{\"pos\":2509350,\"text\":\"/n//n/I will define a Scala function. \"},{\"pos\":2518455,\"text\":\"It uses a def keyword like Python. \"},{\"pos\":2579532,\"text\":\"No return statement is necessary. If the last line is an expression, it will be returned. \"},{\"pos\":2597230,\"text\":\"/n//n/I will create something called an RDD. \"},{\"pos\":2610030,\"text\":\"Resilient distributed dataset. \"},{\"pos\":2616960,\"text\":\"There is a global object called sc - standing for sparkContext. \"},{\"pos\":2649458,\"text\":\"We can say sc.parallelize and pass it a sequence. \"},{\"pos\":2663188,\"text\":\"Seq is the generic sequence. \"},{\"pos\":2680802,\"text\":\"Note the val keyword - you declare constants using the val keyword. \"},{\"pos\":2688109,\"text\":\"There is also a var keyword that lets you declare changeable values. \"},{\"pos\":2719023,\"text\":\"/n//n/A lot of the power is unlocked when you use vals instead of vars. \"},{\"pos\":2742371,\"text\":\"Now we map with function triple. \"},{\"pos\":2768918,\"text\":\"We get a MapPartitionRDD as the return type. \"},{\"pos\":2773973,\"text\":\"/n//n/Has any calculation taken place so far? \"},{\"pos\":2783215,\"text\":\"Turns out no. \"},{\"pos\":2803771,\"text\":\"Spark is lazy in a good way -  it doesn't do any calculations until it has to. \"},{\"pos\":2815932,\"text\":\"I call a method called collect - now it actually does calculations. \"},{\"pos\":2830459,\"text\":\"/n//n/What I just showed you was the driver. \"},{\"pos\":2863638,\"text\":\"It is the orchestrator of all the work that the cluster does. \"},{\"pos\":2866039,\"text\":\"The driver is a process that will have a sparkContext object. \"},{\"pos\":2884636,\"text\":\"In this case, it can run on your laptop. \"},{\"pos\":2898391,\"text\":\"The driver itself do any work - it gets nodes in the cluster to do the work. \"},{\"pos\":2916901,\"text\":\"/n//n/The driver is going to have machines to do the work - physical or virtual. \"},{\"pos\":2930974,\"text\":\"The machines are called workers. \"},{\"pos\":2937892,\"text\":\"Each one will have an executor process. \"},{\"pos\":2948190,\"text\":\"Worker is a machine, while executor is a process. \"},{\"pos\":2966981,\"text\":\"<span class=\\\"t\\\">Resilient Distributed Datasets (RDDs)</span>\"},{\"pos\":2969211,\"text\":\"- Write programs in terms of operations on distributed datasets\"},{\"pos\":2988915,\"text\":\"/n/- partitioned collections of objects spread across a cluster, stored in memory or on disk\"},{\"pos\":2992811,\"text\":\"/n/- RDDs built and manipulated through a diverse set of parallel transformations (map, filter, join) and actions (count, collect, save)\"},{\"pos\":2992812,\"text\":\"/n/- RDDs automatically rebuilt on machine failure\"},{\"pos\":3072647,\"text\":\"/n//n/Let us talk about the internal structure. \"},{\"pos\":3081306,\"text\":\"Data is segmented into partitions. \"},{\"pos\":3089150,\"text\":\"In Spark, the partition is the atom of data. \"},{\"pos\":3110951,\"text\":\"/n//n/Here we have 3 machines each with an executor. \"},{\"pos\":3128092,\"text\":\"Data has 5 partitions, 25 items. \"},{\"pos\":3144543,\"text\":\"These 5 partitions are distributed among the 3 worker machines. \"},{\"pos\":3167032,\"text\":\"/n//n/How do you create these? \"},{\"pos\":3171496,\"text\":\"This happens for you automatically. \"},{\"pos\":3176764,\"text\":\"You can tune the partitioning, but there are defaults. \"},{\"pos\":3198986,\"text\":\"/n//n/There are 2 ways - parallelize a collection. \"},{\"pos\":3243709,\"text\":\"It is useful when learning. \"},{\"pos\":3262305,\"text\":\"/n//n/You can load it from an external datasource - S3, Cassandra, HDFS etc. \"},{\"pos\":3286764,\"text\":\"<span class=\\\"t\\\">Parallelize</span>\"},{\"pos\":3292263,\"text\":\"Notice that in Python syntax is a little different. \"},{\"pos\":3381155,\"text\":\"<span class=\\\"t\\\">Read from text file</span>\"},{\"pos\":3435280,\"text\":\"<span class=\\\"t\\\">Operations on distributed data</span>\"},{\"pos\":3439724,\"text\":\"- Two types of operations: transformations and actions\"},{\"pos\":3444921,\"text\":\"/n/ - transformations are lazy (not computed immediately)\"},{\"pos\":3487153,\"text\":\"/n/ - transformations are executed when an action is run\"},{\"pos\":3500675,\"text\":\"/n/ - persist (cache) distributed data in memory or disk\"},{\"pos\":3530385,\"text\":\"<span class=\\\"t\\\">Lineage of operations</span>\"},{\"pos\":3560260,\"text\":\"A graph of operations. \"},{\"pos\":3565893,\"text\":\"If one of the node goes down, Spark can go back to source, and use the chain of transformations to recalculate. \"},{\"pos\":3586333,\"text\":\"/n//n/At the top, you see a sequence of HDFS blocks. \"},{\"pos\":3606458,\"text\":\"When you create an RDD, each HDFS block becomes a partition. \"},{\"pos\":3647575,\"text\":\"/n//n/Use the collect method after your transformations (an action). \"},{\"pos\":3664013,\"text\":\"Now the cluster will start doing work. \"},{\"pos\":3670768,\"text\":\"It executes a DAG - directed acyclic graph. \"},{\"pos\":3710590,\"text\":\"/n//n/We will wrap up and go to the first lab. \"},{\"pos\":3721639,\"text\":\"<span class=\\\"t\\\">Logical</span>\"},{\"pos\":3721734,\"text\":\"What you specify. \"},{\"pos\":3724405,\"text\":\"<span class=\\\"t\\\">Physical plan</span>\"},{\"pos\":3728167,\"text\":\"What is actually executed.\"},{\"pos\":3731351,\"text\":\"/n//n/Start with HDFS blocks. \"},{\"pos\":3739145,\"text\":\"You can have tangential things - a complex graph. \"},{\"pos\":3771398,\"text\":\"<span class=\\\"t\\\">Lifecycle of a Spark program</span>\"},{\"pos\":3780850,\"text\":\"1. Create some input RDDs from external data or parallelize a collection in your driver program\"},{\"pos\":3783380,\"text\":\"/n/2. Lazily transform them to define new RDDs using transformations like filter() or map()\"},{\"pos\":3783381,\"text\":\"/n/3. Ask Spark to cache() any intermediate RDDs that will need to be reused\"},{\"pos\":3783382,\"text\":\"/n/4. Launch actions such as count() and collect() to kick off a parallel computation, which is then optimized and executed by Spark. \"},{\"pos\":3867814,\"text\":\"<span class=\\\"t\\\">Transformations</span>\"},{\"pos\":3871823,\"text\":\"<span class=\\\"t\\\">Actions</span>\"}]","css":".t{font-weight:bold;}.t:before{content:\"\\A\\A\";white-space: pre;}.t:after{content:\"\\A\";white-space: pre;}","videoid":"OheiUl_uXwo","title":"Training  Apache Spark Essentials","duration":7259.101,"category":"Apache Spark","pageName":""}