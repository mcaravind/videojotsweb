{"text":"[{\"pos\":15411,\"text\":\"In grad school, I started writing Spark streaming as my research project. \"},{\"pos\":16377,\"text\":\"I am one of the members of the project management committee of Apache Spark. \"},{\"pos\":19442,\"text\":\"Now it is pretty widely used, which is pretty gratifying for me. \"},{\"pos\":23515,\"text\":\"I am a software engineer at Databricks and involved with all things streaming in Spark. \"},{\"pos\":30424,\"text\":\"Today I will do a deeper dive into why we built it.\"},{\"pos\":41357,\"text\":\"And a little bit into what's under the hood, how it runs. \"},{\"pos\":42511,\"text\":\"<span class=\\\"t\\\">Streaming in Apache Spark</span>\"},{\"pos\":53736,\"text\":\"We introduced a lot of new concepts which revolutionized how people wrote streaming apps. \"},{\"pos\":62670,\"text\":\"We brought in the idea that you can write functional, concise APIs for expressing streaming computation, not the usual verbose MapReduce stuff. \"},{\"pos\":77177,\"text\":\"/n//n/We also introduced the concept of state management, the user should not have to worry about dealing with fault tolerance of state and managing of state. \"},{\"pos\":89018,\"text\":\"/n//n/More fundamentally, it introduced the concept that a single system Spark can actually handle both batch and stream processing at the same time. \"},{\"pos\":102788,\"text\":\"/n//n/That's why last year, in a survey, we found more than 50% of users found streaming an important part of Apache Spark. \"},{\"pos\":116319,\"text\":\"/n//n/But in the last 2-3 years, we have learnt a lot and we realized streaming apps are growing a lot more complex. \"},{\"pos\":134079,\"text\":\"The main reason is that streaming applications don't run in isolation. They have to interact with a lot of other things, batch data, sql, hdfs, s3. \"},{\"pos\":148000,\"text\":\"You have to do other workloads like interactive analysis, machine learning etc. \"},{\"pos\":158408,\"text\":\"<span class=\\\"t\\\">Use case: IoT device monitoring</span>\"},{\"pos\":169741,\"text\":\"First you want to store all data into some form of long term storage, HDFS, database, S3. \"},{\"pos\":180187,\"text\":\"You need to solve the problem of not just writing to database, but you need to be careful about things like - you don't want data loss in the process. \"},{\"pos\":189034,\"text\":\"Failures should not lead to data loss. \"},{\"pos\":191847,\"text\":\"You do not want to write duplicates. \"},{\"pos\":195400,\"text\":\"You want end-to-end write-once guarantees. \"},{\"pos\":204707,\"text\":\"/n//n/In addition, you also want to do monitoring to identify any weird sensor behavior, if there are any anamolies. \"},{\"pos\":220309,\"text\":\"So you have to do more complex aggregations based on event time, take late data arrivals into account and things like that. \"},{\"pos\":243784,\"text\":\"/n//n/You also want to do a lot more complicated anomaly detection - for that you need to incorporate machine learning. \"},{\"pos\":254279,\"text\":\"And for that you need to build models on how sensors behave, and that means you have to take data in long term storage and build machine learning models on top of it. \"},{\"pos\":265983,\"text\":\"And use the ML model in live streaming to identify anomalies as soon as possible. \"},{\"pos\":278950,\"text\":\"/n//n/You want to do continuous learning too and update the model as you use it for anomaly detection. \"},{\"pos\":291915,\"text\":\"/n//n/If there is an issue, you want to dig deeper interactively on the data that is coming in and identify the problem sensor and root cause etc. \"},{\"pos\":314241,\"text\":\"<span class=\\\"t\\\">Continuous applications: Not just streaming any more</span>\"},{\"pos\":325587,\"text\":\"These applications require more holistic approach to building these sort of continuously running stack of applications. \"},{\"pos\":332016,\"text\":\"/n/1. As we worked with developers, we realized the DStream had some pain points. \"},{\"pos\":342628,\"text\":\"Processing with event-time, dealing with late data. \"},{\"pos\":370399,\"text\":\"DStream API exposes batch time, hard to incorporate event time. \"},{\"pos\":374752,\"text\":\"/n/2. Interoperate streaming with batch AND interactive. \"},{\"pos\":386616,\"text\":\"RDD/DStream has similar API, but still requires translation. \"},{\"pos\":405097,\"text\":\"/n/3. Reasoning about end to end guarantees. \"},{\"pos\":415199,\"text\":\"/n/-Requires carefully constructing sinks that handle failures correctly\"},{\"pos\":418291,\"text\":\"/n/-Data consistency in the storage while being updated\"},{\"pos\":420124,\"text\":\"/n//n/So we gathered all this data accumulated from actual user experience, went back to the drawing board and really thought about the best possible thing to make the life of the end user as simple as possible. \"},{\"pos\":456577,\"text\":\"<span class=\\\"t\\\">Structured Streaming</span>\"},{\"pos\":459224,\"text\":\"Structured streaming was the culmination of the whole thing. \"},{\"pos\":459225,\"text\":\"/n//n/The simplest way to perform streaming analytics is not having to reason about streaming at all. \"},{\"pos\":487404,\"text\":\"/n//n/So Spark should be responsible for reasoning about fault tolerance, the corner cases, late data etc and user should be relieved from that. \"},{\"pos\":496669,\"text\":\"/n//n/We are introducing a new way to think about streaming. \"},{\"pos\":501400,\"text\":\"<span class=\\\"t\\\">New Model</span>\"},{\"pos\":506154,\"text\":\"If you think about a stream, it is no different from a table which is continuously appended to. \"},{\"pos\":506632,\"text\":\"/n//n/Input: data from source as an append only table\"},{\"pos\":511388,\"text\":\"Its like an infinite table. \"},{\"pos\":518563,\"text\":\"/n//n/Trigger: how frequently to check input for new data\"},{\"pos\":519294,\"text\":\"If you check the table every second, you will that the table is continuously growing. \"},{\"pos\":536652,\"text\":\"/n//n/Query: operations on input - usual map/filter/reduce, new window, session ops\"},{\"pos\":546050,\"text\":\"/n//n/Result: final operated table updated every trigger interval\"},{\"pos\":562808,\"text\":\"/n//n/Output: what part of result to write to data sink after every trigger\"},{\"pos\":594028,\"text\":\"/n//n/Complete output mode: write full result table every time. \"},{\"pos\":603855,\"text\":\"/n/Delta output mode: write only the rows that changed in result from previous batch\"},{\"pos\":612637,\"text\":\"/n/Append output mode: write only new rows\"},{\"pos\":635476,\"text\":\"<span class=\\\"t\\\">API - dataset-dataframe</span>\"},{\"pos\":643024,\"text\":\"Essentially the same as batch. \"},{\"pos\":649640,\"text\":\"The dataset/dataframe can represent both static bounded data and completely unbounded data. \"},{\"pos\":658870,\"text\":\"You don't have to learn a new API between batch and streaming. \"},{\"pos\":664479,\"text\":\"<span class=\\\"t\\\">Batch ETL with DataFrames</span>\"},{\"pos\":680469,\"text\":\"Batch: Read from json file, select some devices, write to parquet file\"},{\"pos\":695344,\"text\":\"/n/To convert to streaming, replace load() with stream(), select does not change, replace save() with startStream(). \"},{\"pos\":734701,\"text\":\"/n//n/Spark will translate this and figure out how to calculate the result. \"},{\"pos\":762869,\"text\":\"<span class=\\\"t\\\">Continuous aggregations</span>\"},{\"pos\":771856,\"text\":\"avg() and groupBy() operations\"},{\"pos\":782687,\"text\":\"<span class=\\\"t\\\">Continuous windowed aggregations</span>\"},{\"pos\":791234,\"text\":\"Calculate averages over sliding window of time. \"},{\"pos\":793547,\"text\":\"Windowing is a sort of grouping, so becomes simple in this API. \"},{\"pos\":839816,\"text\":\"/n//n/This simplifies event-time stream processing - not possible in DStreams. \"},{\"pos\":850713,\"text\":\"Also works in batch, so you can apply same window operations on batch jobs as well. \"},{\"pos\":860844,\"text\":\"<span class=\\\"t\\\">Joining streams with static data</span>\"},{\"pos\":870808,\"text\":\"Combining batch and streaming data together is easier. \"},{\"pos\":921259,\"text\":\"<span class=\\\"t\\\">Output modes</span>\"},{\"pos\":927171,\"text\":\"Defines what is outputted every time there is a trigger. \"},{\"pos\":942347,\"text\":\"/n/Append mode with non-aggregation queries\"},{\"pos\":951244,\"text\":\"/n/Complete mode with aggregation queries. \"},{\"pos\":975585,\"text\":\"<span class=\\\"t\\\">Query management</span>\"},{\"pos\":980606,\"text\":\"Improvement over DStreams - very strong query management APIs. \"},{\"pos\":994233,\"text\":\"You can define when to stop a query, restart it, interactively list, get exceptions etc. \"},{\"pos\":1029801,\"text\":\"<span class=\\\"t\\\">Query Execution</span>\"},{\"pos\":1035793,\"text\":\"Logically: \"},{\"pos\":1039181,\"text\":\"/n/DataSet operations on table (i.e. as easy to understand as batch)\"},{\"pos\":1041956,\"text\":\"/n/Physically:\"},{\"pos\":1045277,\"text\":\"/n/Spark automatically runs the query in streaming fashion (i.e. incrementally and continuously)\"},{\"pos\":1057989,\"text\":\"<span class=\\\"t\\\">Structured streaming</span>\"},{\"pos\":1061047,\"text\":\"High level streaming API built on Datasets/DataFrames\"},{\"pos\":1061048,\"text\":\"/n/Unifies streaming, interactive and batch queries\"},{\"pos\":1088999,\"text\":\"<span class=\\\"t\\\">What can you do with this that's hard with other engines?</span>\"},{\"pos\":1091454,\"text\":\"True unification - express business logic once, but run that code on both batch and streaming data. \"},{\"pos\":1118880,\"text\":\"/n/Flexible API tightly integrated with the engine. \"},{\"pos\":1142675,\"text\":\"/n/The standard benefits of Spark\"},{\"pos\":1162939,\"text\":\"<span class=\\\"t\\\">Underneath the Hood</span>\"},{\"pos\":1175964,\"text\":\"<span class=\\\"t\\\">Batch Execution on Spark SQL</span>\"},{\"pos\":1187676,\"text\":\"A logical plan is the abstract representation of the query. \"},{\"pos\":1199964,\"text\":\"The planner does some magic to convert the logical plan to construct an optimized physical plan. \"},{\"pos\":1247717,\"text\":\"<span class=\\\"t\\\">Continuous incremental execution</span>\"},{\"pos\":1260031,\"text\":\"Planner knows how to convert streaming logical plans to a continuous series of incremental execution plans, for each processing the next chunk of streaming data. \"},{\"pos\":1280160,\"text\":\"/n//n/Every trigger interval, planner polls for new data from sources. \"},{\"pos\":1295464,\"text\":\"Incrementally executes new data and writes to sink. \"},{\"pos\":1324259,\"text\":\"/n//n/Maintain running aggregate as in-memory state backed by WAL in file system for fault tolerance. \"},{\"pos\":1368282,\"text\":\"It keeps the state in memory backed by files and next incremental execution uses the state to keep maintaining running aggregate. \"},{\"pos\":1382977,\"text\":\"<span class=\\\"t\\\">Fault tolerance</span>\"},{\"pos\":1387454,\"text\":\"All data and metadata in the system needs to be recoverable/replayable. \"},{\"pos\":1408127,\"text\":\"The planner is fault tolerant - tracks offsets by writing the offset range of each execution to a write ahead log (WAL) in HDFS. \"},{\"pos\":1423843,\"text\":\"If planner fails, it can read those offsets from WAL and rerun the exact - regenerate the incremental execution that had failed. \"},{\"pos\":1441197,\"text\":\"/n//n/Fault tolerant sources: structured streaming sources are by design replayable (e.g. Kafka, Kinesis, files) and generate exactly the same data given offsets recovered by planner. \"},{\"pos\":1471383,\"text\":\"/n//n/Fault-tolerant state: intermediate \\\"state data\\\" is maintained in versioned, key-value maps in Spark workers, backed by HDFS. \"},{\"pos\":1478241,\"text\":\"Planner makes sure \\\"correct version\\\" of state used to re-execute after failure. \"},{\"pos\":1483109,\"text\":\"/n//n/Fault tolerant sink: sinks are by design idempotent, and handles re-executions to avoid double committing the output. \"},{\"pos\":1520024,\"text\":\"/n//n/Since all these components are fault tolerant, you get end to end exactly once guarantees in this streaming pipeline. \"},{\"pos\":1550091,\"text\":\"<span class=\\\"t\\\">Release plan: Spark 2.0</span>\"},{\"pos\":1563337,\"text\":\"It is an experimental release for streaming use cases. \"},{\"pos\":1576681,\"text\":\"It sets the future direction and good to start working with it right now to plan for the future. \"},{\"pos\":1589535,\"text\":\"/n//n/Kafka coming soon after 2.0 release. \"},{\"pos\":1613937,\"text\":\"Sinks: files and in-memory table. \"},{\"pos\":1635607,\"text\":\"<span class=\\\"t\\\">Spark 2.1+</span>\"},{\"pos\":1641015,\"text\":\"Stability and scalability\"},{\"pos\":1654411,\"text\":\"/n/Support for more queries: multiple aggregations, sessionization, more output modes, watermarks and late data. \"},{\"pos\":1695345,\"text\":\"/n/Sources and sinks will get public APIs\"},{\"pos\":1709287,\"text\":\"/n/Working very closely with ML side of Spark to make sure the new API integrates nicely with the cool stuff that is present in ML. \"},{\"pos\":1730079,\"text\":\"<span class=\\\"t\\\">Try Apache Spark with Databricks</span>\"}]","css":".t{font-weight:bold;}.t:before{content:\"\\A\\A\";white-space: pre;}.t:after{content:\"\\A\";white-space: pre;}","videoid":"rl8dIzTpxrI","title":"A Deep Dive Into Structured Streaming","duration":1764.801,"category":"Apache Spark","pageName":""}