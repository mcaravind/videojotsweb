{"text":"[{\"pos\":5641,\"text\":\"My name is Mark Grover, with me is Ted Malaska. \"},{\"pos\":14050,\"text\":\"Ted is a principal solutions architect at Cloudera and I am a software engineer. \"},{\"pos\":16258,\"text\":\"The most common question we get in our presentation - where are the slides? \"},{\"pos\":26171,\"text\":\"It is on the link on the slide deck. \"},{\"pos\":39336,\"text\":\"<span class=\\\"t\\\">About the book</span>\"},{\"pos\":52410,\"text\":\"Talks about architecture, Kafka and so on. \"},{\"pos\":64139,\"text\":\"We talk about mistakes people make in this presentation. \"},{\"pos\":72534,\"text\":\"<span class=\\\"t\\\">Mistake #1</span>\"},{\"pos\":75362,\"text\":\"Deciding on the number of executors, cores and memory. \"},{\"pos\":118217,\"text\":\"<span class=\\\"t\\\">Spark architecture recap</span>\"},{\"pos\":156100,\"text\":\"<span class=\\\"t\\\">Answer #1 - most granular</span>\"},{\"pos\":166840,\"text\":\"Have smallest sized executors\"},{\"pos\":185682,\"text\":\"/n//n/Turns out that is the wrong answe\"},{\"pos\":188159,\"text\":\"<span class=\\\"t\\\">Why?</span>\"},{\"pos\":189739,\"text\":\"You are not using the benefits of running multiple tasks in the same executor. \"},{\"pos\":198227,\"text\":\"<span class=\\\"t\\\">Answer #2: Least granular</span>\"},{\"pos\":211152,\"text\":\"Give it all the memory and all the cores in the cluster. \"},{\"pos\":212511,\"text\":\"/n//n/Turns out that is the wrong answer. \"},{\"pos\":213356,\"text\":\"<span class=\\\"t\\\">Why?</span>\"},{\"pos\":215518,\"text\":\"Because we need to leave some overhead for OS/Hadoop daemons to run. \"},{\"pos\":218301,\"text\":\"<span class=\\\"t\\\">Answer #3 - with overhead</span>\"},{\"pos\":220848,\"text\":\"Leave out one core per node and leave out 1G of RAM from the node and we give the rest to the executor. \"},{\"pos\":240073,\"text\":\"/n//n/That is also the wrong answer. \"},{\"pos\":248462,\"text\":\"<span class=\\\"t\\\">Lets assume..</span>\"},{\"pos\":255351,\"text\":\"You are running Spark on YARN\"},{\"pos\":256781,\"text\":\"/n/There are 3 things we need to talk about to get to the right answer. \"},{\"pos\":260497,\"text\":\"<span class=\\\"t\\\">1 - memory overhead</span>\"},{\"pos\":312786,\"text\":\"<span class=\\\"t\\\">2 - YARN AM needs a core - client mode</span>\"},{\"pos\":334150,\"text\":\"Cluster mode. \"},{\"pos\":364096,\"text\":\"<span class=\\\"t\\\">3-HDFS throughput</span>\"},{\"pos\":376532,\"text\":\"Best to keep under 5 cores per executors. \"},{\"pos\":381538,\"text\":\"<span class=\\\"t\\\">Calculations</span>\"},{\"pos\":432489,\"text\":\"<span class=\\\"t\\\">Correct answer</span>\"},{\"pos\":442080,\"text\":\"Not etched in stone. \"},{\"pos\":468567,\"text\":\"/n//n/If you have lots of IO, your CPU is not the culprit. You can up your cores. \"},{\"pos\":494636,\"text\":\"<span class=\\\"t\\\">Dynamic allocation helps with this though, right?</span>\"},{\"pos\":509784,\"text\":\"Dynamic allocation allows Spark to dynamically scale the cluster resources allocated to your application based on the workload. \"},{\"pos\":513556,\"text\":\"Works with Spark on Yarn only. \"},{\"pos\":524248.00000000006,\"text\":\"/n//n/But it can you dynamically figure out the number of executors you need, but not with the number of cores for each executor or the memory for each executor. \"},{\"pos\":563087,\"text\":\"<span class=\\\"t\\\">Mistake #2</span>\"},{\"pos\":564863,\"text\":\"<span class=\\\"t\\\">Application failure</span>\"},{\"pos\":571940,\"text\":\"<span class=\\\"t\\\">Why?</span>\"},{\"pos\":574813,\"text\":\"No Spark shuffle block can be greater than 2GB. \"},{\"pos\":582876,\"text\":\"<span class=\\\"t\\\">What's a shuffle block again?</span>\"},{\"pos\":591939,\"text\":\"In MapReduce terminology, a file written from one Mapper for a Reducer. \"},{\"pos\":598719,\"text\":\"/n/The Reducer makes a local copy of this file and then 'reduces' it\"},{\"pos\":606776,\"text\":\"<span class=\\\"t\\\">Defining shuffle and partition</span>\"},{\"pos\":630193,\"text\":\"<span class=\\\"t\\\">Once again</span>\"},{\"pos\":632502,\"text\":\"You will get an overflow exception if shuffle block size &gt; 2GB. \"},{\"pos\":636855,\"text\":\"<span class=\\\"t\\\">Whats going on here?</span>\"},{\"pos\":642057,\"text\":\"Spark uses ByteBuffer as abstraction for blocks\"},{\"pos\":655425,\"text\":\"<span class=\\\"t\\\">Spark SQL</span>\"},{\"pos\":659683,\"text\":\"Default number of partitions to use when doing shuffles is 200. \"},{\"pos\":667630,\"text\":\"<span class=\\\"t\\\">So what can I do?</span>\"},{\"pos\":673262,\"text\":\"1. Increase the number of partitions - thereby reducing the average partition size\"},{\"pos\":681005,\"text\":\"2. Get rid of skew in your data - more later\"},{\"pos\":690845,\"text\":\"<span class=\\\"t\\\">How exactly?</span>\"},{\"pos\":695541,\"text\":\"In Spark SQL, increase the value of spark.sql.shuffle.partitions\"},{\"pos\":700445,\"text\":\"/n/In regular Spark applications, use rdd.repartition() or rdd.coalesce()\"},{\"pos\":704975,\"text\":\"<span class=\\\"t\\\">How many partitions should I have</span>\"},{\"pos\":712911,\"text\":\"Rule of thumb is around 128MB per partition\"},{\"pos\":716981,\"text\":\"<span class=\\\"t\\\">There's more</span>\"},{\"pos\":721424,\"text\":\"Spark uses a different data structure for bookkeeping during shuffles, when the number of partitions is less than 2000 vs more than 2000. \"},{\"pos\":734601,\"text\":\"/n//n/This 2000 number is hard coded in Spark. \"},{\"pos\":760216,\"text\":\"<span class=\\\"t\\\">So what are you saying?</span>\"},{\"pos\":767810,\"text\":\"If number of partitions &lt; 2000, but not by much, bump it to be slightly higher than 2000. \"},{\"pos\":769118,\"text\":\"<span class=\\\"t\\\">Can you summarize?</span>\"},{\"pos\":775734,\"text\":\"Dont have too big partitions - your job will fail due to 2 GB limit\"},{\"pos\":776601,\"text\":\"/n/Dont have too few partitions - your job will be slow, not making use of parallelism\"},{\"pos\":780086,\"text\":\"/n/Rule of thumb ~ 128 MB per partition\"},{\"pos\":783951,\"text\":\"/n/If partitions &lt; 2000, but close, bump to just &gt; 2000\"},{\"pos\":794109,\"text\":\"<span class=\\\"t\\\">Mistake #3</span>\"},{\"pos\":798869,\"text\":\"<span class=\\\"t\\\">Slow jobs on Join/Shuffle</span>\"},{\"pos\":806034,\"text\":\"Your dataset takes 20 seconds to run over a with a map job, but takes 4 hours when joined or shuffled. What went wrong?\"},{\"pos\":809881,\"text\":\"<span class=\\\"t\\\">Mistake - skew</span>\"},{\"pos\":822679,\"text\":\"Skew happens because you have a NULL key, or you have a popular value. \"},{\"pos\":837101,\"text\":\"All your work will be going through a single core. \"},{\"pos\":839823,\"text\":\"<span class=\\\"t\\\">Mistake - skew: answers</span>\"},{\"pos\":864163,\"text\":\"Salting\"},{\"pos\":873821,\"text\":\"/n/Normal key: 'foo'\"},{\"pos\":880308,\"text\":\"/n/Salted key: 'foo' + random.nextInt(saltFactor)\"},{\"pos\":894863,\"text\":\"<span class=\\\"t\\\">Managing parallelism</span>\"},{\"pos\":912161,\"text\":\"Before and after salting. \"},{\"pos\":945945,\"text\":\"/n//n/Two stage aggregation\"},{\"pos\":954158,\"text\":\"/n/-stage one to do operations on the salted keys\"},{\"pos\":958806,\"text\":\"/n/-stage two to do operation access unsalted key results\"},{\"pos\":976027,\"text\":\"<span class=\\\"t\\\">Mistake - skew: isolated salting</span>\"},{\"pos\":984179,\"text\":\"Second stage only required for isolated keys. \"},{\"pos\":988665,\"text\":\"<span class=\\\"t\\\">Mistake - skew: isolated map join</span>\"},{\"pos\":1000521,\"text\":\"<span class=\\\"t\\\">Managing parallelism</span>\"},{\"pos\":1021317,\"text\":\"Cartesian Join\"},{\"pos\":1027591,\"text\":\"/n//n/How to fight cartesian join\"},{\"pos\":1035358,\"text\":\"/n/-Nested structures. \"},{\"pos\":1069426,\"text\":\"/n//n/This is going to be multiple magnitudes faster than doing a Cartesian. \"},{\"pos\":1072568,\"text\":\"/n//n/Repartitioning\"},{\"pos\":1104666,\"text\":\"<span class=\\\"t\\\">Mistake #4</span>\"},{\"pos\":1109331,\"text\":\"<span class=\\\"t\\\">Out of luck?</span>\"},{\"pos\":1114154,\"text\":\"Do you ever run out of memory? \"},{\"pos\":1117068,\"text\":\"/n/Do you ever have more than 20 stages?\"},{\"pos\":1117069,\"text\":\"/n/Is your driver doing a lot of work?\"},{\"pos\":1119534,\"text\":\"<span class=\\\"t\\\">Mistake - DAG management</span>\"},{\"pos\":1126806,\"text\":\"Shuffles are to be avoided\"},{\"pos\":1133254,\"text\":\"/n/ReduceByKey over GroupByKey\"},{\"pos\":1142645,\"text\":\"/n/TreeReduce over Reduce\"},{\"pos\":1147285,\"text\":\"/n/Use Complex/Nested Types\"},{\"pos\":1275750,\"text\":\"<span class=\\\"t\\\">Mistake - DAG management: shuffles</span>\"},{\"pos\":1275751,\"text\":\"Map side reduction, where possible\"},{\"pos\":1275752,\"text\":\"/n/Think about partitioning/bucketing ahead of time\"},{\"pos\":1275753,\"text\":\"/n/Do as much as possible with a single shuffle\"},{\"pos\":1275754,\"text\":\"/n/Only send what you have to send\"},{\"pos\":1275755,\"text\":\"/n/Avoid skew and cartesians\"},{\"pos\":1277506,\"text\":\"<span class=\\\"t\\\">ReduceByKey over GroupByKey</span>\"},{\"pos\":1279194,\"text\":\"ReduceByKey can do almost anything that GroupByKey can do\"},{\"pos\":1279195,\"text\":\"/n/-aggregations\"},{\"pos\":1279196,\"text\":\"/n/-windowing\"},{\"pos\":1279197,\"text\":\"/n/-use memory\"},{\"pos\":1279198,\"text\":\"/n/-but you have more control\"},{\"pos\":1279199,\"text\":\"/n/ReduceByKey has a fixed limit of Memory requirements\"},{\"pos\":1279200,\"text\":\"/n/GroupByKey is unbounded and dependent on data\"},{\"pos\":1280284,\"text\":\"<span class=\\\"t\\\">TreeReduce over Reduce</span>\"},{\"pos\":1284091,\"text\":\"TreeReduce and Reduce return some result to driver\"},{\"pos\":1284092,\"text\":\"/n/TreeReduce does more work on the executors\"},{\"pos\":1284093,\"text\":\"/n/While Reduce bring everything back to the driver\"},{\"pos\":1307385,\"text\":\"<span class=\\\"t\\\">Complex Types</span>\"},{\"pos\":1356555,\"text\":\"<span class=\\\"t\\\">Mistake #5</span>\"},{\"pos\":1363091,\"text\":\"<span class=\\\"t\\\">Ever seen this?</span>\"},{\"pos\":1367529,\"text\":\"Method not found exception. \"},{\"pos\":1396824,\"text\":\"<span class=\\\"t\\\">Shading</span>\"},{\"pos\":1407261,\"text\":\"Bring your own dependency using shadedPattern. \"},{\"pos\":1431679,\"text\":\"<span class=\\\"t\\\">Future of shading</span>\"},{\"pos\":1458898,\"text\":\"Spark 2 has some libraries shaded\"},{\"pos\":1467758,\"text\":\"/n/-Gauva is fully shaded\"},{\"pos\":1477454,\"text\":\"<span class=\\\"t\\\">Summary</span>\"},{\"pos\":1482175,\"text\":\"<span class=\\\"t\\\">5 mistakes</span>\"},{\"pos\":1487006,\"text\":\"Size up your executors right\"},{\"pos\":1491123,\"text\":\"/n/2 GB limit on Spark shuffle blocks\"},{\"pos\":1495128,\"text\":\"/n/Skew and cartesians are evil\"},{\"pos\":1496755,\"text\":\"/n/Learn to manage your DAG\"},{\"pos\":1498876,\"text\":\"/n/Do shady stuff, don't let classpath leaks mess you up\"},{\"pos\":1543188,\"text\":\"<span class=\\\"t\\\">Questions</span>\"},{\"pos\":1557834,\"text\":\"<span class=\\\"t\\\">Some of the sophistication from auto-generation could be built into the product. How is that coming along?</span>\"},{\"pos\":1589492,\"text\":\"I guess we are trudging through it. \"},{\"pos\":1596929,\"text\":\"Some cannot be built in the auto-generated part. \"},{\"pos\":1611672,\"text\":\"/n//n/Some of the DAG management will get better as the query optimizer gets better. \"},{\"pos\":1627802,\"text\":\"Skew is very hard to fix with zero cost. \"},{\"pos\":1657151,\"text\":\"<span class=\\\"t\\\">Do you think GroupByKey is overly demonized? Do you think there is no use case where it is better than ReduceByKey?</span>\"},{\"pos\":1669249,\"text\":\"You can do everything GroupByKey does with ReduceByKey. \"},{\"pos\":1671946,\"text\":\"The reason why I demonize it is because I don't want a phone call at night time. \"},{\"pos\":1680063,\"text\":\"/n//n/Unless I have super strong guarantees on the data that's coming in it could cause trouble. \"},{\"pos\":1745222,\"text\":\"<span class=\\\"t\\\">How do you set your Python worker memory in your Spark?</span>\"},{\"pos\":1759334,\"text\":\"We are both Scala people, sorry. \"},{\"pos\":1768425,\"text\":\"But I don't think there is any significant performance degradation in Python. \"}]","css":".t{font-weight:bold;}.t:before{content:\"\\A\\A\";white-space: pre;}.t:after{content:\"\\A\";white-space: pre;}","videoid":"vfiJQ7wg81Y","title":"Top 5 Mistakes When Writing Spark Applications","duration":1777.961,"category":"Apache Spark","pageName":""}